
[To-dos (contains research
questions)](https://docs.google.com/document/d/1vTpxr06BUZmbJGiVwWKPF25CAjFsIY7gwTVsOitZcGI/edit)

[Meetings con
Paolo](https://docs.google.com/document/d/1MMMNPR3i4zqfXu89s6NPBws8MVdwPz0GxWhJaZTk59o/edit)

# Initial set up

Set working directory
```{r setup}
knitr::opts_knit$set(root.dir = "C:/Users/benji/Desktop/Regrid")
```

Load packages
```{r}
library(sf)
library(tidyverse)
library(terra)
library(exactextractr)
library(stringdist)
library(openxlsx)
library(tigris)
```

Set universal variables
```{r}
crs = 3310
cropland_threshold = 0.1
feet_to_metres = 0.3048
ha_to_m2 = 10000
```

# Import and process data

## Central Valley counties

Get polygons for central valley counties
```{r, eval = F}
central_valley_counties = str_to_lower(c("Butte", "Colusa", "Glenn", "Fresno", "Kern", "Kings", "Madera", "Merced", "Placer", "Sacramento", "San_Joaquin", "Shasta", "Stanislaus", "Sutter", "Tehama", "Tulare", "Yolo", "Yuba"))

ca_counties = st_transform(counties(state = "CA", year = 2020, class = "sf"), crs)%>%
  filter(NAME %in% str_to_title(central_valley_counties) | NAME == "San Joaquin")

st_write(ca_counties, "Data/Central Valley counties/central_valley_counties.gpkg")
```

## GW basins

Import spatial data for Bulletin 118 Groundwater Basin Boundaries (obtained from [CA DWR](https://water.ca.gov/programs/groundwater-management/bulletin-118))

Filter for only gw basins intersecting with Central Valley counties

```{r, eval = F}
gw_basins = st_transform(st_read("Data/GW basins/Raw/i08_B118_v6_2.shp"), crs)%>%
  dplyr::select(1:4)%>%
  rename(basin_number = 1, subbasin_number = 2, basin_name = 3, subbasin_name = 4)%>%
  filter(basin_name %in% c("SACRAMENTO VALLEY", "SAN JOAQUIN VALLEY"))

st_write(gw_basins, "Data/GW basins/Filtered/filtered_gw_basins.gpkg")
```


## Crop raster

Crop raster from [2023 Cropland CROS (USDA)](https://croplandcros.scinet.usda.gov/). Import and reclassify to binary (cropland or not cropland)

```{r, eval = F}
cropland_extent_raster = rast("Crop data/crops croplandcros/crops_3311.TIF")
levels(cropland_extent_raster)
reclass_matrix = cbind(c(59,92,111,112,121,122,123,124,131,141,142,143,152,176,190,195), 0)

cropland_extent_raster_binary = classify(cropland_extent_raster, reclass_matrix, others = 1)

writeRaster(cropland_extent_raster_binary, "Crop data/crops croplandcros/cropland_binary.tif")
```


## Cropland parcels

Generate list of all Regrid Central Valley land parcel geopackages

```{r, eval = F}
gpkg_files = list.files("Data/Parcels/Raw/", pattern = "\\.gpkg$", full.names = T)

central_valley_gpkgs = Filter(function(file) any(sapply(central_valley_counties, function(county) grepl(county, file))), gpkg_files)
```

Keep only relevant columns. Descriptions of Regrid column names [here](https://docs.google.com/spreadsheets/d/14RcBKyiEGa7q-SR0rFnDHVcovb9uegPJ3sfb3WlNPc0/edit#gid=942435131)

```{r}
#colnames(st_read("Data/Parcels/Raw/ca_yuba.gpkg")%>%data.frame())

columns = c("parcelnumb", "parcelnumb_no_formatting", "alt_parcelnumb1", "alt_parcelnumb2", 
             "alt_parcelnumb3", "usedesc", "parvaltype", "improvval", "landval", "parval", 
             "agval", "saleprice", "saledate", "owntype", "owner", "ownfrst", "ownlast", 
             "owner2", "owner3", "owner4", "mailadd", "mail_address2", "careof", 
             "mail_addno", "mail_addpref", "mail_addstr", "mail_addsttyp", "mail_addstsuf", 
             "mail_unit", "mail_city", "mail_state2", "mail_zip", "address", "address2", 
             "saddno", "saddpref", "saddstr", "saddsttyp", "saddstsuf", "sunit", "scity", 
             "original_address", "city", "county", "szip", "cdl_raw", "cdl_majority_category", 
             "cdl_majority_percent", "cdl_date", "ll_gisacre")
```

Go through the parcels geopackage for each Central Valley county, filtering for only parcels intersecting the gw basins of interest. For each parcel, calculate the fraction of the parcel that is cropland (based on our binary cropland raster above). Label parcels as "cropland parcels" if at least 10% of the parcel area is designated as cropland. This is to avoid designating parcels as cropland parcels in cases where a tiny portion is cropland or a few grid cells are mistakenly classified as cropland in the Cropland CROS raster. Combine cropland parcels from all Central Valley counties into one large dataset of cropland parcels.

```{r, eval = F}
start_time = Sys.time()

master_cropland_parcels = lapply(central_valley_gpkgs, function(file) {
  county_name = str_to_title(gsub("_", " ", gsub("ca_|\\.gpkg", "", basename(file))))
  cat(county_name, "County\n")

  return(st_transform(st_read(file, quiet = T), crs) %>%
    dplyr::select(all_of(columns))%>%
    st_filter(., gw_basins, .pred = st_intersects)%>%
    mutate(fraction_cropland = exact_extract(cropland_extent_raster_binary, ., 'mean'), 
         parcel_area_ha = ll_gisacre * 0.404686, 
         cropland_parcel = ifelse(fraction_cropland > cropland_threshold, 1, 0))%>%
    filter(cropland_parcel == 1))})

master_cropland_parcels = do.call(rbind, master_cropland_parcels)%>%
  separate(address, into = c("address_no", "address_st"), sep = " ", extra = "merge", convert = T, remove = F)%>%
  rename(parcel_id = parcelnumb)

end_time = Sys.time()
end_time - start_time

st_write(master_cropland_parcels, "Data/Parcels/Cropland parcels/cropland_parcels.gpkg", delete_dsn = T)
```


## Sections

Import sections (1x1 mile squares) from [California Data Portal](https://data.ca.gov/dataset/public-land-survey-system-plss-sections).
This is because most of the wells are not precisely geolocated, but rather snapped to the centroid of the section that they fall within.

Filter for sections that intersect at least 1 cropland parcel and intersect the GW basins of interest
```{r, eval = F}
sections = st_transform(st_read("Data/Sections/Raw data/Public_Land_Survey_System_(PLSS)_3A_Sections.shp"), crs)%>%
  st_filter(., master_cropland_parcels, .pred = st_intersects)%>%
  st_filter(., gw_basins, .pred = st_intersects)%>%
  dplyr::select(MTRS, geometry)
```

## Well data

Import well data from the [USGWD](https://www.hydroshare.org/resource/8b02895f02c14dd1a749bcc5584a5c55/) dataset and from the [OSWCR](https://data.ca.gov/dataset/well-completion-reports) dataset (maintained by the CA Dept of Water Resources).

Import well data and get rid of duplicates (based on CA well ID).

```{r, eval = F}
usgwd = read.csv("Data/Wells/Raw/Chung-Yi's dataset/USGWD_California.csv")%>%
  arrange(rowSums(is.na(.)))%>%
  distinct(Well.ID..State., .keep_all = T)%>%
  rename(ca_well_id = 2)
```

Bring in OSWCR dataset for additional important covariates that are not in USGWD, such as well address details, accuracy of geolocation, etc.

```{r, eval = F}
oswcr = read.csv("Data/Wells/Raw/OSWCR/wellcompletionreports.csv")%>%
  dplyr::select(1, LLACCURACY, METHODOFDETERMINATIONLL, WELLLOCATION, CITY, PLANNEDUSEFORMERUSE, APN)%>%
  rename(ca_well_id = 1, geo_accuracy = 2, method_of_determination = 3, well_address = 4, city = 5, use_oswcr = 6)
```

Combine the 2 well datasets and turn into spatial point data

Clean up the IDs, addresses, and such (ex: turn addresses to lowercase so capitalisation won't trip up any of the well-parcel matching)

```{r, eval = F}
filtered_wells = st_transform(st_as_sf(left_join(usgwd, oswcr, by = "ca_well_id")%>%
                                   filter(!is.na(Longitude) & !is.na(Latitude)), 
                                 coords = c("Longitude", "Latitude"), crs = 4326), crs)%>%
  filter(grepl("Irrigation|Unknown", USGS.Water.Use.Category))%>%
  mutate_at(vars(well_address, city), ~ na_if(., ""))%>%
  separate(well_address, into = c("address_no", "address_st"), sep = " ", extra = "merge", convert = T, remove = F)%>%
  st_join(., sections)%>%
  filter(!is.na(MTRS))%>%
  mutate(APN = gsub("[- ]", "", APN),
         well_address = str_to_lower(well_address),
         address_st = str_to_lower(address_st))

rm(oswcr, wells)

st_write(filtered_wells, "Data/Wells/Processed/filtered_wells_with_section.gpkg", delete_dsn = T)
```


We want to make one last update to our sections dataset: Before, we filtered for sections intersecting cropland parcels and groundwater basins of interest. Now, we want to narrow down further to those sections that also have a well in them. This will be our final 'filtered sections' dataset.

```{r, eval = F}
filtered_sections = sections%>%
  filter(MTRS %in% filtered_wells$MTRS)

rm(sections)

st_write(filtered_sections, "Data/Sections/Filtered/filtered_sections.gpkg", delete_dsn = T)
```

Not all wells are snapped to the centroid of the section they fall in. Based on the 'geo_accuracy' column, we can pull out the wells with accuracy of coordinates 50ft or less and put a buffer around them for the later matching steps. For example, for a well with accuracy 2.5ft, we can only consider matching that well to parcels within a 2.5ft buffer around that well.

```{r}
distances_less_than_50_ft = c("0.1 Ft", "10 Ft", "2.5 Ft", "20 Ft", "5 Ft", "50 Ft")

well_buffer = filtered_wells%>%
  filter(geo_accuracy %in% distances_less_than_50_ft)%>%
  mutate(buffer_distance = as.numeric(gsub("\\s*\\w*$", "", geo_accuracy))*feet_to_metres)%>%
  mutate(geometry = st_buffer(geometry, dist = buffer_distance))%>%
  dplyr::select(ca_well_id, MTRS, geometry)%>%
  rename(well_MTRS = MTRS)

st_write(well_buffer, "Data/Wells/Processed/well_buffer.gpkg", delete_dsn = T)
```

## Filtered parcels

Our master cropland parcels dataset will be great for analysis just concerning land (ex: relationship between precipitation and farm size).
However, for land-water analysis, some of the wells could be matched to non-cropland parcels. We'll ultimately want to exclude these wells/parcels, but we don't want to mistakenly match a well with a cropland parcel when its 'true' match was with a non-cropland parcel. Therefore, for future matching steps, we want to get a dataset of all parcels (cropland and non-cropland) that intersect any sections with wells in them (aka, our final 'filtered sections' dataset).

Here, similar to the cropland parcel iteration above, we go through each county. We filter for parcels that intersect with our 'filtered_sections' dataset and designate parcels as cropland parcels or not. We do some processing on parcel addresses to help with our matches by address later on. Finally, we combine the filtered parcels from all Central Valley counties into one dataset!

We also want to make sure that each parcel has a unique ID, since the parcel IDs in the Regrid dataset can be duplicated in some cases.

```{r, eval = F}
start_time = Sys.time()

filtered_parcels = lapply(central_valley_gpkgs, function(file) {
  county_name = str_to_title(gsub("_", " ", gsub("ca_|\\.gpkg", "", basename(file))))
  cat(county_name, "County\n")

  return(st_transform(st_read(file, quiet = T), crs) %>%
    dplyr::select(all_of(columns))%>%
    st_filter(., filtered_sections, .pred = st_intersects)%>%
    mutate(fraction_cropland = exact_extract(cropland_extent_raster_binary, ., 'mean'), 
         parcel_area_ha = ll_gisacre * 0.404686, 
         cropland_parcel = ifelse(fraction_cropland > cropland_threshold, 1, 0))%>%
    separate(address, into = c("address_no", "address_st"), sep = " ", extra = "merge", convert = T, remove = F)%>%
    rename(parcel_id = parcelnumb))})

filtered_parcels = do.call(rbind, filtered_parcels)%>%
  mutate(my_parcel_num = row_number())

end_time = Sys.time()
end_time - start_time

st_write(filtered_parcels, "Data/Parcels/Filtered parcels/filtered_parcels.gpkg", delete_dsn = T)
```


Now, let's run a spatial join of these filtered parcels with the section they intersect. 

If a parcel intersects 2 sections, this will create a duplicate row for that parcel (ie: the parcel will have two rows, one for each section it intersects). We want this because we want to consider all possible sections a parcel could intersect rather than just the one it has most overlap with.

Also, we want to clean up the IDs, addresses, and such (ex: turn addresses to lowercase so capitalisation won't trip up any of the well-parcel matching)

```{r, eval = F}
filtered_parcels_with_section = filtered_parcels%>%
  st_join(., filtered_sections)%>%
  mutate(parcel_id = gsub("[- ]", "", parcel_id), 
         parcelnumb_no_formatting = gsub("[- ]", "", parcelnumb_no_formatting),
         address = str_to_lower(address),
         mailadd = str_to_lower(mailadd),
         address_st = str_to_lower(address_st),
         owner = str_to_lower(owner))

st_write(filtered_parcels_with_section, "Data/Parcels/Filtered parcels/filtered_parcels_with_sections.gpkg", delete_dsn = T)
```

Now, using a spatial join, let's create a dataset of parcels that intersect with the designated buffer of any of the wells that had a geolocation geoaccuracy 50ft or less (which are make up the 'well_buffer' dataset). In cases where there are multiple rows for a given parcel, only keep the row where the section code (MTRS) matches that of the well whose buffer the parcel intersects.

```{r, eval = F}
well_buffer_parcels = st_filter(filtered_parcels_with_section, well_buffer%>%select(ca_well_id, well_MTRS), .pred = st_intersects)%>%
  st_join(., well_buffer)%>%
  data.frame()%>%
  filter(MTRS == well_MTRS)

st_write(well_buffer_parcels, "Data/Parcels/Filtered parcels/parcels_intersecting_well_buffer.gpkg", delete_dsn = T)
```

```{r}
well_buffer_parcels = st_read("Data/Parcels/Filtered parcels/parcels_intersecting_well_buffer.gpkg")
well_buffer_parcels_ids = unique(well_buffer_parcels$ca_well_id)
```

Lastly, let's update our cropland parcels dataset using spatial joins, so that it contains columns for the gw basin and section that the parcel has the largest overlap with.
```{r, eval = F}
master_cropland_parcels_covariates = master_cropland_parcels%>%
  st_join(., gw_basins%>%select(basin_name:subbasin_name), join = st_intersects, largest = T)%>%
  st_join(., filtered_sections%>%select(MTRS)%>%rename(parcel_MTRS = 1), join = st_intersects, largest = T)

st_write(master_cropland_parcels_covariates, "Data/Parcels/Cropland parcels/master_cropland_parcels_w_covariates.gpkg", row.names = F)
```

# Matching wells with parcels

## Matching loop

Create an empty dataframe for each well and the section it falls in that we will populate with the parcel number it gets matched with (if any) and the matching method.

```{r, eval = F}
loop_matches = data.frame(my_parcel_num = NA, ca_well_id = filtered_wells$ca_well_id, string_dist = NA, match_method = NA, section = filtered_wells$MTRS)%>%
  mutate(row_id = row_number())

duplicate_parcels_list = list()
```

Loop through each well in our filtered well dataset. For each well, generate a dataframe of all the parcels that intersect with that well's section or the buffer around that well if applicable.

Then, for each well, we go through our hierarchy for matching methods:
1. First priority is if the well's section or buffer only has one parcel in it (which makes it easy for us!).
2. Second is if the APN (Assessor Parcel Numbers) of the well and parcel address perfectly match.
3. Third is if well address is a perfect match with a parcel in that section's address or mailing address.
4. Fourth is if the number in the well and parcel address number is the exact same and the street name is very similar (less than 0.3 based on Jaro-Winkler string distance)

Export our completed match dataset
```{r, eval = F}
for (i in 1:nrow(filtered_wells)) {
  
  well = filtered_wells[i, ]
  
  #Get intersecting parcels
  if (well$ca_well_id %in% well_buffer_parcels_ids) {
    intersecting_parcels = filter(well_buffer_parcels, ca_well_id == well$ca_well_id)}
  else {
    intersecting_parcels = filter(filtered_parcels_with_section, MTRS == well$MTRS)}
  
  #Route 1: section only has 1 parcel in it
  if (grepl("Irrigation", well$USGS.Water.Use.Category)){
    intersecting_parcels_route1 = filter(intersecting_parcels, cropland_parcel == 1)}
  else{intersecting_parcels_route1 = intersecting_parcels}
  
  if (nrow(intersecting_parcels_route1) == 1) {
    loop_matches$my_parcel_num[i] = intersecting_parcels_route1$my_parcel_num
    loop_matches$match_method[i] = if (well$ca_well_id %in% well_buffer_parcels_ids) "1 parcel in well buffer" else "1 parcel in section"
    next} 
  
  #Route 2: APN match
  if (!is.na(well$well_address) & well$well_address != ""){
    apn_match = intersecting_parcels$parcel_id == well$APN | intersecting_parcels$parcelnumb_no_formatting == well$APN | intersecting_parcels$parcelnumb_no_formatting == sub("000$", "", well$APN)
  apn_match[is.na(apn_match)] = FALSE
    if (any(apn_match, na.rm = T)) {
      loop_matches$my_parcel_num[i] = intersecting_parcels$my_parcel_num[apn_match][1]
      loop_matches$match_method[i] = "APN match"
      
      if (length(intersecting_parcels$my_parcel_num[apn_match]) > 1) {
        for (parcel_id in intersecting_parcels$my_parcel_num[apn_match]) {
          duplicate_parcels_list[[length(duplicate_parcels_list) + 1]] = data.frame(type = "APN", id = parcel_id, i = i)}}
      next}}
  
  #Route 3: well address is a perfect match with parcel address or mail address of parcel in section
  address_match <- intersecting_parcels$address == well$well_address | intersecting_parcels$mailadd == well$well_address
  address_match[is.na(address_match)] = F
  if (any(address_match, na.rm = T)) {
    loop_matches$my_parcel_num[i] <- intersecting_parcels$my_parcel_num[address_match][1]
    loop_matches$match_method[i] <- "Perfect address match"
    
    if (length(intersecting_parcels$my_parcel_num[address_match]) > 1) {
        for (parcel_id in intersecting_parcels$my_parcel_num[address_match]) {
          duplicate_parcels_list[[length(duplicate_parcels_list) + 1]] = data.frame(type = "Address", id = parcel_id, i = i)}}
      next}
  
  # Route 4: string distance
  if (!is.na(well$address_no) & !is.na(well$address_st)){
    intersecting_parcels = intersecting_parcels%>%
      filter(address_st != "" & !is.na(address_st) & address_no != "0")
  
    address_number_match = intersecting_parcels$address_no == well$address_no
    address_number_match[is.na(address_number_match)] = F
    if (any(address_number_match, na.rm = T)) {
      distances = stringdist(well$address_st, intersecting_parcels$address_st[address_number_match], method = "jw")
      min_dist = which.min(distances)
  
      if (distances[min_dist] < 0.3) {
        loop_matches$my_parcel_num[i] <- intersecting_parcels$my_parcel_num[address_number_match][min_dist]
        loop_matches$match_method[i] <- "String distance match"
        loop_matches$string_dist[i] <- distances[min_dist]
        next}}}}

write.csv(loop_matches, "Data/Well-parcel matches/full_loop_matches.csv", row.names = F)
```

Let's run some quick diagnostics

```{r}
table(loop_matches%>%filter(!is.na(my_parcel_num))%>%
  left_join(., filtered_wells%>%select(ca_well_id, APN), by = "ca_well_id")%>%
  left_join(., filtered_parcels_with_section%>%select(my_parcel_num, parcel_id, parcelnumb_no_formatting, MTRS), by = c("my_parcel_num" = "my_parcel_num", "section" = "MTRS"))%>%
  pull(match_method))

loop_matches%>%filter(!is.na(my_parcel_num))%>%
  left_join(., filtered_wells%>%select(ca_well_id, APN), by = "ca_well_id")%>%
  left_join(., filtered_parcels_with_section%>%select(my_parcel_num, parcel_id, parcelnumb_no_formatting, MTRS), by = c("my_parcel_num" = "my_parcel_num", "section" = "MTRS"))
```

## Geocoding

The fifth and final method we'll used to match wells and parcels is geocoding well addresses. 

Prepare and export any remaining unmatched wells as an excel file to read into ArcGIS for geocoding. Filter out any wells whose addresses are ungeocodable (ex: any part of the address is NA)

```{r, eval = F}
wells_to_geocode = filtered_wells%>%
  filter(ca_well_id %in% (loop_matches%>%filter(is.na(my_parcel_num))%>%pull(ca_well_id)) & !is.na(well_address) & address_no != 0 & grepl("^[0-9]+$", address_no) & !is.na(city))%>%
  mutate(state = "CA")

write.xlsx(wells_to_geocode%>%select(ca_well_id, well_address, city, state), 'Well-parcel matches/Geocoding/wells_to_geocode.xlsx', row.names = FALSE)
```

Conducted the geocoding in ArcGIS Pro, yielding a point shapefile. We can now import this shapefile back into R and only keep wells that got filtered
to an actual address (rather than an intersection, POI, etc., which would mess up our spatial join with parcels). 


```{r, eval = F}
geocoded_wells = st_transform(st_read("Data/Well-parcel matches/Geocoding/Geocoded well addresses/geocoded_wells.shp"), crs = crs)%>%
  filter(!is.na(Addr_type) & !Addr_type %in% c("StreetName", "StreetMidBlock", "Locality", "StreetInt", "POI") & Score > 90)%>%
  select(USER_ca_we)%>%
  rename(ca_well_id = 1)
```

Spatially join geocoded wells to the respective parcels they fall in
```{r, eval = F}
geocoded_wells_joined = geocoded_wells%>%
  st_join(., filtered_parcels%>%select(my_parcel_num), join = st_within)%>%
  filter(!is.na(my_parcel_num))%>%
  rename(my_parcel_num_geocoded = 2)%>%
  data.frame()%>%select(-geometry)
```

## Combine all matches together

Join matches with geocoded matches. For any wells that didn't get matched but had a geocode match, use geocoded parcel number, otherwise keep original match.

```{r, eval = F}
final_matches = loop_matches%>%
  left_join(., geocoded_wells_joined, by = "ca_well_id") %>%
  mutate(
    match_method = ifelse(is.na(my_parcel_num) & !is.na(my_parcel_num_geocoded), "Geocoded", match_method),
    my_parcel_num = ifelse(is.na(my_parcel_num), my_parcel_num_geocoded, my_parcel_num))%>%
  filter(!is.na(my_parcel_num))

write.csv(final_matches, "Data/Well-parcel matches/all_matches.csv", row.names = F)
```


# Final processing

## Combine datasets and covariates

Now we want to join in all relevant info. Bring in relevant parcel data by joining by my_parcel_num. Bring in relevant well data by joining by ca_well_id.

First, prepare relevant parcel data to join to matches. We want to filter for only parcels whose parcel_numbers are in our final match dataset. Then, we can join each parcel to the GW basin and subbasin that it has the largest overlap with. Finally, we select only the variables of interest for subsequent analysis. I run the spatial join here as opposed to when I initally created filtered_parcels because running the spatial joins only on matched parcels means the spatial join is less computationally intensive. 

```{r}
filtered_parcels_with_covariates = filtered_parcels%>%
  filter(my_parcel_num %in% final_matches$my_parcel_num)%>%
  st_join(., gw_basins, join = st_intersects, largest = T)%>%
  st_join(., filtered_sections%>%select(MTRS)%>%rename(parcel_MTRS = 1), join = st_intersects, largest = T)%>%
  select(usedesc:agval, owner:owner4, cdl_majority_category:cdl_majority_percent, fraction_cropland:my_parcel_num, basin_name:parcel_MTRS)
```

Now we can join the final match dataset with the relevant parcel and well data (for wells, we also select for only the variables of interest), yielding our final match dataset with all covariates included! We also filter for only wells in cropland parcels.

```{r}
final_matches_joined = final_matches%>%
  left_join(., filtered_wells%>%
              select(ca_well_id, Well.Depth..Feet., Well.Capacity..GPM., Year.Well.was.Constructed,
                     USGS.Water.Use.Category), 
            by = "ca_well_id")%>%
  left_join(., filtered_parcels_with_covariates, by = "my_parcel_num")%>%
  filter(cropland_parcel == 1)%>%
  rename(well_depth_ft = Well.Depth..Feet., well_capacity_gpm = Well.Capacity..GPM., well_year = Year.Well.was.Constructed)
```

## Filter out public land

Exclude public well-parcel owners from datasets

To do so, let's see who the largest landowners are, and we can exclude those that are evidently public land
```{r}
final_matches_joined%>%
  filter(!grepl(exclude_owners, owner) & !is.na(owner))%>%
  group_by(owner)%>%
  summarise(area_ha = sum(parcel_area_ha))%>%
  arrange(-area_ha)
```

Based on a scan of this table, we can make a list of owners or types of owners that we want to classify as public land, such as counties, school districts, University of California, etc.
```{r}
exclude_owners = c("U S A|CITY OF|UNIVERSITY OF|COUNTY OF|STATE OF|SAN JOSE UNIFIED|USA 101|CALIFORNIA STATE|U C D|POSTAL SERVICE|SANITATION DIST|WATER DIST")
```

Now, we can exclude these public landowners from our datasets. With our filtered dataset, we can then aggregate land ownership and well covariates by owner and subbasin (ex: if one owner has two farms near each other, they would technically show up as separate parcels in our dataset, but we want to consider both parcels under one owner). 

Filter out publicly owned land, aggregate parcels by owner and subbasin, and created version of the dataset with key variables normalised by subbasin.

Finally, we can calculate land area and land value ownership percentiles (ex: 99th percentile is the top percentile of owners who own the most land)

```{r}
final_matches_joined_filtered_owners = final_matches_joined%>%
  filter(!grepl(exclude_owners, owner) & !is.na(owner))%>%
  group_by(owner, subbasin_name)%>%
  reframe(
    
    #change any zeroes, infities, etc. to NA 
    well_depth_ft = sum(well_depth_ft, na.rm = T), well_depth_ft = replace(well_depth_ft, well_depth_ft %in% c(0, Inf, -Inf), NA), 
    well_capacity_gpm = sum(well_capacity_gpm, na.rm = T), well_capacity_gpm = replace(well_capacity_gpm, well_capacity_gpm %in% c(0, Inf, -Inf), NA),
    newest_well = as.numeric(min(well_year, na.rm = T)),
  
    #change missing land values from 0 to NA
    parcel_area_ha = sum(parcel_area_ha, na.rm = T),
    ag_val = sum(agval, na.rm = T), ag_val = ifelse(ag_val == 0, NA, ag_val),
    improved_val = sum(improvval, na.rm = T), improved_val = ifelse(improved_val == 0, NA, improved_val),
    land_val = sum(landval, na.rm = T), land_val = ifelse(landval == 0, NA, landval))%>%
  
  ungroup()%>%
  
  mutate(area_percentile = ntile(parcel_area_ha, 100),
         land_val_percentile = ntile(land_val, 100),
         improved_val_percentile = ntile(improved_val, 100),
         ag_val_percentile = ntile(ag_val, 100))

write.csv(final_matches_joined_filtered_owners, "Data/Well-parcel matches/final_matches_joined_filtered_owners.csv", row.names = F)

paste("After filtering out wells in publicly-owned land, our matched dataset consists of", nrow(final_matches_joined_filtered_owners), "well-parcel matches")
```

We also want to do this for our cropland parcels layer
```{r}
cropland_parcels_filtered_owners = master_cropland_parcels_covariates%>%
  filter(!grepl(exclude_owners, owner) & !is.na(owner))%>%
  group_by(owner, subbasin_name)%>%
  summarise(parcel_area_ha = sum(parcel_area_ha, na.rm = T),
            improvval = sum(improvval, na.rm = T),
            landval = sum(landval, na.rm = T),
            agval = sum(agval, na.rm = T))%>%
  mutate(across(where(is.numeric), ~ na_if(., 0)))

write.csv(cropland_parcels_filtered_owners%>%data.frame()%>%select(-geom), "Data/Parcels/Cropland parcels/cropland_parcels_filtered_owners.csv", row.names = F)
```


### Normalise variables

Variables of interest, such as well depth and well yield, might systematically vary across geographies (ex: due to geology or topology, parcels in one subbasin might have on average deeper wells, regardless of parcel size, than another subbasin). Therefore, as a robustness check, we want to normalise these variables, which we can accomplish by calculating a z-score for each well within the subbasin it is located in. 

We exclude any wells in subbasins with less than 30 wells, as per the Central Limit Theorem, the z-scores in these subbasins wouldn't be useful as a normalisation procedure.

```{r}
final_matches_joined_normalised = final_matches_joined_filtered_owners%>%
  group_by(subbasin_name)%>%
  mutate(subbasin_n = n(), 
         depth_z_score = (well_depth_ft - mean(well_depth_ft, na.rm = T)) / sd(well_depth_ft, na.rm = T),
         area_z_score = (parcel_area_ha - mean(parcel_area_ha, na.rm = T)) / sd(parcel_area_ha, na.rm = T),
         yield_z_score = (well_capacity_gpm - mean(well_capacity_gpm, na.rm = T)) / sd(well_capacity_gpm, na.rm = T),
         well_year_z_score = (newest_well - mean(newest_well, na.rm = T)) / sd(newest_well, na.rm = T),
         agval_z_score = (ag_val - mean(ag_val, na.rm = T)) / sd(ag_val, na.rm = T),
         landval_z_score = (land_val - mean(land_val, na.rm = T)) / sd(land_val, na.rm = T),
         improved_val_z_score = (improved_val - mean(improved_val, na.rm = T)) / sd(improved_val, na.rm = T),
         area_percentile_z = ntile(area_z_score, 100))%>%
  ungroup()%>%
  filter(subbasin_n >= 30)

write.csv(final_matches_joined_normalised, "Data/Well-parcel matches/final_matches_joined_normalised.csv", row.names = F)
```

Again, let's do this for cropland parcels too

```{r}
cropland_parcels_normalised = cropland_parcels_filtered_owners%>%
  group_by(subbasin_name)%>%
  mutate(subbasin_n = n(),
         area_z_score = (parcel_area_ha - mean(parcel_area_ha, na.rm = T)) / sd(parcel_area_ha, na.rm = T))%>%
  ungroup()%>%
  filter(subbasin_n >= 30)%>%
  mutate(area_z_score = area_z_score - min(area_z_score))

write.csv(cropland_parcels_normalised%>%data.frame()%>%select(-geom), "Data/Parcels/Cropland parcels/cropland_parcels_normalised.csv", row.names = F)
```
